{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyNvtNICFt2+5VS29GusRb30",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/DOLLARDEV05/AI-LAB/blob/main/Attention_is_All_You_Need/Transformers/single_head_attention.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 166,
      "metadata": {
        "id": "7VJMpX4pVY7F",
        "outputId": "75e5ab45-c243-457f-b3d1-e99e3721369e",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Tokens: ['my', 'cat', 'is', 'lazy']\n",
            "Vocab: {'my': 0, 'cat': 1, 'lazy': 2, 'is': 3}\n",
            "Token IDs: [0 1 3 2]\n"
          ]
        }
      ],
      "source": [
        "import numpy as np\n",
        "\n",
        "sentence = \"My cat is lazy\"\n",
        "\n",
        "tokens = sentence.lower().split()\n",
        "print(\"Tokens:\", tokens)\n",
        "\n",
        "vocab = {word: idx for idx, word in enumerate(set(tokens))}\n",
        "print(\"Vocab:\", vocab)\n",
        "\n",
        "token_ids = np.array([vocab[word] for word in tokens])\n",
        "print(\"Token IDs:\", token_ids)\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# let's create some embeddings!\n",
        "\n",
        "d_model = 4\n",
        "\n",
        "# this will create a random matrix of small float values with dimension being\n",
        "# rows = length of the sentence/token list, columns being the dimension we set\n",
        "# that is the d_model = 4 where this is a hyperparameter, and for single head\n",
        "# attention this is d_model = Q = K = V\n",
        "embedding = np.random.rand(len(tokens),d_model)\n",
        "\n",
        "print(embedding)"
      ],
      "metadata": {
        "id": "-3v1CdbtWc9C",
        "outputId": "01e6af5a-d6c3-4caf-ff8b-2d24e2072b16",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 167,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[[0.48285766 0.29490128 0.39238593 0.51395501]\n",
            " [0.83773367 0.25914538 0.46919931 0.26134735]\n",
            " [0.19616678 0.77443857 0.53184078 0.44215419]\n",
            " [0.18016639 0.04269236 0.4988755  0.29426143]]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "embedding_token = np.array([embedding[vocab[token]] for token in tokens])\n",
        "print(embedding_token)"
      ],
      "metadata": {
        "id": "fMoIlxdIPhan",
        "outputId": "75a44686-f3b9-42ec-e67f-be4e0c4bbddc",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 168,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[[0.48285766 0.29490128 0.39238593 0.51395501]\n",
            " [0.83773367 0.25914538 0.46919931 0.26134735]\n",
            " [0.18016639 0.04269236 0.4988755  0.29426143]\n",
            " [0.19616678 0.77443857 0.53184078 0.44215419]]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# let's convert the maths of positional encoding into code successful\n",
        "\n",
        "import math\n",
        "\n",
        "for embedding in range(len(embedding_token)):\n",
        "  for value in range(d_model):\n",
        "    if value % 2 == 0:\n",
        "      temp = math.sin(embedding/10000**((2*(value//2))/d_model))\n",
        "      embedding_token[embedding,value]=embedding_token[embedding,value]+temp\n",
        "    elif value % 2 == 1:\n",
        "      temp = math.cos(embedding/10000**((2*(value//2))/d_model))\n",
        "      embedding_token[embedding,value]=embedding_token[embedding,value]+temp\n",
        "    else:\n",
        "      print(\"error\")\n",
        "print(embedding_token)"
      ],
      "metadata": {
        "id": "4GnTjTGQ25aa",
        "outputId": "8ec31957-c966-43e8-940e-55c3308051d5",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 169,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[[ 0.48285766  1.29490128  0.39238593  1.51395501]\n",
            " [ 1.67920465  0.79944769  0.47919914  1.26129735]\n",
            " [ 1.08946382 -0.37345448  0.51887416  1.29406144]\n",
            " [ 0.33728679 -0.21555392  0.56183628  1.44170422]]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "x = 0 // 2\n",
        "y = 1 // 2\n",
        "z = 2 // 2\n",
        "p = 3 // 2\n",
        "q = 4 // 2\n",
        "\n",
        "# how positional encoding uses floor division to have similar values given to the\n",
        "# sin and cos fuction to have pi/2 shifted sin and cos waves that also dipicts a\n",
        "# circle idk if this is necessary to mention here\n",
        "\n",
        "print(x,y,z,p,q)\n",
        "\n",
        "# was just showcasing where i was stuck in positional encoding and why it matter"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5sAVXWAssM0h",
        "outputId": "a2bce835-acd9-4172-c907-cd68c4faa831"
      },
      "execution_count": 170,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "0 0 1 1 2\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# let's work on making the weight matrix that will extract features from the\n",
        "# embedding matrix ! W_Q,W_K,W_V\n",
        "\n",
        "\n",
        "W_Q = np.random.rand(d_model,d_model)\n",
        "W_K = np.random.rand(d_model,d_model)\n",
        "W_V = np.random.rand(d_model,d_model)\n",
        "\n",
        "print(W_Q)\n",
        "print(W_K)\n",
        "print(W_V)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "GZHoI9NayTjz",
        "outputId": "aae16091-aaa8-4a4f-b1ac-ef91b0b36987"
      },
      "execution_count": 171,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[[0.92779748 0.46298449 0.57184952 0.98870354]\n",
            " [0.81959304 0.63445143 0.83566886 0.57654681]\n",
            " [0.70924652 0.16211957 0.34625257 0.48766587]\n",
            " [0.65388826 0.66949956 0.4178588  0.39708335]]\n",
            "[[0.53056113 0.95352246 0.76062017 0.50652536]\n",
            " [0.56784912 0.5532496  0.00905934 0.24741354]\n",
            " [0.51644729 0.5815917  0.05461971 0.6075039 ]\n",
            " [0.51707516 0.71346211 0.0821721  0.71180803]]\n",
            "[[0.6487974  0.38440954 0.33164392 0.33597358]\n",
            " [0.70734113 0.90493853 0.83437099 0.77436826]\n",
            " [0.39469849 0.42910212 0.8181538  0.27323931]\n",
            " [0.21832438 0.57199189 0.56816157 0.70284576]]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# multiplying the weighted matrix with the embedded_token to get Q K V matrices\n",
        "\n",
        "\n",
        "Q = embedding_token @ W_Q\n",
        "K = embedding_token @ W_K\n",
        "V = embedding_token @ W_V\n",
        "\n",
        "print(Q,\"\\n\\n\",K,\"\\n\\n\\n\",V,\"\\n\\n\\n\\n\")\n",
        "\n",
        "# i used the ** 1/2 or **0.5 which is fine for scalers and one time operations\n",
        "# but its better to use np.sqrt() which is faster for sqrting whole arrays!\n",
        "# print((Q @ K.T)/d_model**(1/2))\n",
        "pre_softmax = (Q @ K.T)/np.sqrt(d_model)\n",
        "\n",
        "print(pre_softmax)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "rFL33XoezneV",
        "outputId": "f94e381b-d2db-47ea-c35b-f09bb8ae9076"
      },
      "execution_count": 172,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[[2.77754196 2.12231323 2.12671465 2.01649383]\n",
            " [3.37780146 2.20678201 2.32129404 2.85568384]\n",
            " [1.91890237 1.21795933 1.03132156 1.62873022]\n",
            " [1.47746122 1.07570505 0.80971164 1.05566484]] \n",
            "\n",
            " [[1.97696925 2.48517717 0.52483909 1.88097707]\n",
            " [2.24455301 3.22203958 1.41429657 2.23727085]\n",
            " [1.30305954 2.05725139 0.95996141 1.69578475]\n",
            " [1.09217742 1.55771605 0.40374956 1.48504772]] \n",
            "\n",
            "\n",
            " [[1.71462115 2.39176478 2.42176797 2.33624999]\n",
            " [2.11945699 2.29603054 2.33261325 2.20066885]\n",
            " [0.9300056  1.0436896  1.20946931 1.12814218]\n",
            " [0.60287575 1.00032147 1.21079659 1.11321279]] \n",
            "\n",
            "\n",
            "\n",
            "\n",
            "[[ 7.83730052 10.29588275  6.72326893  5.09638983]\n",
            " [ 9.37591778 12.18196493  8.0061798   6.15237323]\n",
            " [ 5.21266888  6.66694823  4.37904867  3.41407718]\n",
            " [ 4.00243134  5.14459241  3.35284395  2.59196309]]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# let's recreate softmax from scratch using only numpy and convert these vectors\n",
        "# into attention scores that our model can use!\n",
        "\n",
        "# done with row max subtraction for each element (was checking row max each\n",
        "# iterations which caused all values other than 1st getting zeroed)\n",
        "\n",
        "# we dont sum all row values and then exponent rather we sum the exponent of\n",
        "# individual values then divide\n",
        "\n",
        "\n",
        "for vector in pre_softmax:\n",
        "  row_max = np.max(vector,axis=0)\n",
        "  for element in range(len(vector)):\n",
        "    vector[element] = np.exp(vector[element] - row_max)\n",
        "  vector /= np.sum(vector,axis=0)\n",
        "\n",
        "\n",
        "softmax = pre_softmax.copy()\n",
        "print(softmax)\n"
      ],
      "metadata": {
        "id": "NCZT49Ea6jl9",
        "outputId": "45b553d5-72d6-4d9d-f3ce-fe5e79b204e7",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 173,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[[0.07644691 0.89352898 0.0250924  0.00493171]\n",
            " [0.05605891 0.92746049 0.01424869 0.00223191]\n",
            " [0.17002748 0.72795512 0.07387244 0.02814496]\n",
            " [0.204089   0.63951959 0.10658777 0.04980363]]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# final attention mechanism done\n",
        "attention = softmax @ V\n",
        "\n",
        "print(attention)"
      ],
      "metadata": {
        "id": "Q-I2rh52OlgX",
        "outputId": "43b765a5-124b-46fa-b41a-0b732bfd6819",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 174,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[[2.05118301 2.26553483 2.30571401 2.17875832]\n",
            " [2.07642933 2.28066117 2.31910404 2.19056017]\n",
            " [1.92007198 2.18332683 2.23322913 2.11388479]\n",
            " [1.83452222 2.1175536  2.1752248  2.05986198]]\n"
          ]
        }
      ]
    }
  ]
}