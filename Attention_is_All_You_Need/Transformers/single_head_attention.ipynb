{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyO58XKqcvYZpKCUPOY16Gtb",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/DOLLARDEV05/AI-LAB/blob/main/Attention_is_All_You_Need/Transformers/single_head_attention.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 172,
      "metadata": {
        "id": "7VJMpX4pVY7F",
        "outputId": "73068e98-6ceb-4db7-a777-9fc2ca85f3a7",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Tokens: ['my', 'cat', 'is', 'lazy']\n",
            "Vocab: {'is': 0, 'cat': 1, 'my': 2, 'lazy': 3}\n",
            "Token IDs: [2 1 0 3]\n"
          ]
        }
      ],
      "source": [
        "import numpy as np\n",
        "\n",
        "sentence = \"My cat is lazy\"\n",
        "\n",
        "tokens = sentence.lower().split()\n",
        "print(\"Tokens:\", tokens)\n",
        "\n",
        "vocab = {word: idx for idx, word in enumerate(set(tokens))}\n",
        "print(\"Vocab:\", vocab)\n",
        "\n",
        "token_ids = np.array([vocab[word] for word in tokens])\n",
        "print(\"Token IDs:\", token_ids)\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# let's create some embeddings!\n",
        "\n",
        "d_model = 4\n",
        "\n",
        "# this will create a random matrix of small float values with dimension being\n",
        "# rows = length of the sentence/token list, columns being the dimension we set\n",
        "# that is the d_model = 4 where this is a hyperparameter, and for single head\n",
        "# attention this is d_model = Q = K = V\n",
        "embedding = np.random.rand(len(tokens),d_model)\n",
        "\n",
        "print(embedding)"
      ],
      "metadata": {
        "id": "-3v1CdbtWc9C",
        "outputId": "b2b45ce4-0c91-498a-b31c-d0f003bc9a04",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 173,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[[0.14586421 0.70119547 0.17144063 0.76415822]\n",
            " [0.25865658 0.47998634 0.72607315 0.80242705]\n",
            " [0.34788698 0.75669908 0.1984486  0.85131991]\n",
            " [0.962738   0.5953954  0.87845373 0.17610556]]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "embedding_token = np.array([embedding[vocab[token]] for token in tokens])\n",
        "print(embedding_token)"
      ],
      "metadata": {
        "id": "fMoIlxdIPhan",
        "outputId": "b8562726-bfe2-439c-8906-d2561800d732",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 174,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[[0.34788698 0.75669908 0.1984486  0.85131991]\n",
            " [0.25865658 0.47998634 0.72607315 0.80242705]\n",
            " [0.14586421 0.70119547 0.17144063 0.76415822]\n",
            " [0.962738   0.5953954  0.87845373 0.17610556]]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# let's convert the maths of positional encoding into code successful\n",
        "\n",
        "import math\n",
        "\n",
        "for embedding in range(len(embedding_token)):\n",
        "  for value in range(d_model):\n",
        "    if value % 2 == 0:\n",
        "      temp = math.sin(embedding/10000**((2*(value//2))/d_model))\n",
        "      embedding_token[embedding,value]=embedding_token[embedding,value]+temp\n",
        "    elif value % 2 == 1:\n",
        "      temp = math.cos(embedding/10000**((2*(value//2))/d_model))\n",
        "      embedding_token[embedding,value]=embedding_token[embedding,value]+temp\n",
        "    else:\n",
        "      print(\"error\")\n",
        "print(embedding_token)"
      ],
      "metadata": {
        "id": "4GnTjTGQ25aa",
        "outputId": "70415385-1a55-4b42-a351-3689074ce9ab",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 175,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[[ 0.34788698  1.75669908  0.1984486   1.85131991]\n",
            " [ 1.10012756  1.02028864  0.73607298  1.80237705]\n",
            " [ 1.05516163  0.28504864  0.1914393   1.76395823]\n",
            " [ 1.103858   -0.3945971   0.90844923  1.17565559]]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "x = 0 // 2\n",
        "y = 1 // 2\n",
        "z = 2 // 2\n",
        "p = 3 // 2\n",
        "q = 4 // 2\n",
        "\n",
        "# how positional encoding uses floor division to have similar values given to the\n",
        "# sin and cos fuction to have pi/2 shifted sin and cos waves that also dipicts a\n",
        "# circle idk if this is necessary to mention here\n",
        "\n",
        "print(x,y,z,p,q)\n",
        "\n",
        "# was just showcasing where i was stuck in positional encoding and why it matter"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5sAVXWAssM0h",
        "outputId": "4511a19f-c4e0-4fa1-8f6a-e5ff80bc9c4d"
      },
      "execution_count": 176,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "0 0 1 1 2\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# let's work on making the weight matrix that will extract features from the\n",
        "# embedding matrix ! W_Q,W_K,W_V\n",
        "\n",
        "\n",
        "W_Q = np.random.rand(d_model,d_model)\n",
        "W_K = np.random.rand(d_model,d_model)\n",
        "W_V = np.random.rand(d_model,d_model)\n",
        "\n",
        "print(W_Q)\n",
        "print(W_K)\n",
        "print(W_V)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "GZHoI9NayTjz",
        "outputId": "22ed38f1-5e90-4a03-827f-22ee8ee928a9"
      },
      "execution_count": 177,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[[0.63014091 0.02741707 0.19828858 0.53166054]\n",
            " [0.80905515 0.03125995 0.95912015 0.42628215]\n",
            " [0.60602925 0.44329526 0.98692131 0.15422808]\n",
            " [0.72422329 0.10867545 0.63136481 0.41348703]]\n",
            "[[0.245192   0.76355019 0.93882039 0.66777096]\n",
            " [0.15809499 0.20076901 0.79161411 0.57583391]\n",
            " [0.31457606 0.80364227 0.56777226 0.90171603]\n",
            " [0.94634113 0.81764359 0.46842689 0.35226859]]\n",
            "[[0.11120649 0.21421011 0.09893766 0.40249899]\n",
            " [0.04748347 0.26406346 0.52640719 0.63207535]\n",
            " [0.04948195 0.8520003  0.51485093 0.70153184]\n",
            " [0.97824882 0.87217952 0.24891423 0.9430594 ]]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# multiplying the weighted matrix with the embedded_token to get Q K V matrices\n",
        "\n",
        "\n",
        "Q = embedding_token @ W_Q\n",
        "K = embedding_token @ W_K\n",
        "V = embedding_token @ W_V\n",
        "\n",
        "print(Q,\"\\n\\n\",K,\"\\n\\n\\n\",V,\"\\n\\n\\n\\n\")\n",
        "\n",
        "# i used the ** 1/2 or **0.5 which is fine for scalers and one time operations\n",
        "# but its better to use np.sqrt() which is faster for sqrting whole arrays!\n",
        "# print((Q @ K.T)/d_model**(1/2))\n",
        "pre_softmax = (Q @ K.T)/np.sqrt(d_model)\n",
        "\n",
        "print(pre_softmax)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "rFL33XoezneV",
        "outputId": "0e744304-cb8c-4957-d2e3-e0769baec62e"
      },
      "execution_count": 178,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[[3.1015189  0.35361672 3.1185789  1.72991037]\n",
            " [3.27011036 0.58422825 3.06112569 1.87860791]\n",
            " [2.28903802 0.31440314 1.78525907 1.44139811]\n",
            " [1.77831924 0.54840561 1.47925188 1.04489476]] \n",
            "\n",
            " [[2.17743179 2.29152145 2.6971128  2.07498187]\n",
            " [2.36825948 3.11008636 3.10270077 2.62079968]\n",
            " [2.0333104  2.45903565 2.15123517 1.66275794]\n",
            " [1.60662095 2.45496359 1.79045674 1.74321171]] \n",
            "\n",
            "\n",
            " [[1.9429726  2.32216252 1.52214949 3.135513  ]\n",
            " [1.97038371 2.70421015 1.4735365  3.30382678]\n",
            " [1.86593876 2.00289179 0.79208389 2.40269186]\n",
            " [1.29905486 1.93164062 0.66184773 1.94090575]] \n",
            "\n",
            "\n",
            "\n",
            "\n",
            "[[ 9.78217895 11.32737289  8.38056279  7.22518102]\n",
            " [10.30674623 11.9913411   8.89732948  7.72185566]\n",
            " [ 6.75530287  7.85779865  5.8323308   5.07927795]\n",
            " [ 5.64334947  6.622623    4.94202484  4.33670501]]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# let's recreate softmax from scratch using only numpy and convert these vectors\n",
        "# into attention scores that our model can use!\n",
        "\n",
        "# done with row max subtraction for each element (was checking row max each\n",
        "# iterations which caused all values other than 1st getting zeroed)\n",
        "\n",
        "# we dont sum all row values and then exponent rather we sum the exponent of\n",
        "# individual values then divide\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "for vector in pre_softmax:\n",
        "  row_max = np.max(vector,axis=0)\n",
        "  for element in range(len(vector)):\n",
        "    vector[element] = np.exp(vector[element] - row_max)\n",
        "\n",
        "  for number in vector:\n",
        "     number = number / np.sum(vector,axis=0)\n",
        "print(pre_softmax)"
      ],
      "metadata": {
        "id": "NCZT49Ea6jl9",
        "collapsed": true,
        "outputId": "28c1cb37-d43f-44ba-da8b-e78175d88dff",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 179,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[[0.21327051 1.         0.05250693 0.01653639]\n",
            " [0.18551958 1.         0.04531978 0.01398898]\n",
            " [0.33204135 1.         0.1319321  0.06213035]\n",
            " [0.37558385 1.         0.18626253 0.10168068]]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "VQMovMzTTKd6"
      },
      "execution_count": 179,
      "outputs": []
    }
  ]
}